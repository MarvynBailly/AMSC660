\documentclass[12pt]{report}
\usepackage{commands}
\usepackage{matlab-prettifier}

\begin{document}

\large
\begin{center}
AMSC 660 Homework 7\\
Due Oct 18\\
By Marvyn Bailly\\
\end{center}
\normalsize
\hrule

%Good luck my man
%---------------%
%---Problem 1---%
%---------------%

\begin{problem}%[vskip]
\subsection*{Problem 1}

Prove that for a sequence of iterates of the conjugate gradient algorithm, the preliminary version, the residuals are orthogonal, i.e.,
\[
     r_k^\top r_i = 0, \quad i=0,1,\dots,k-1.
\]
You can use the facts proven in class:
\[
     \text{span}\brac{p_0,\dots,p_k} = \text{span}\brac{r_0,\dots,r_k} = \text{span}\brac{r_0,Ar_0,\dots,A^k r_0} = \mathcal{K}(r_0,k),
\]
and
\[
     p_k^\top Ap_i = 0, \quad i=0,1,\dots,k-1,
\]
and Theorem 5.2 from NW.


\subsection*{Solution}
\begin{proof}

We wish to prove that for a sequence of iterates of the conjugate gradient algorithm, the preliminary version, the residuals are orthogonal. From Theorem 5.2 we have that,
\begin{equation} \label{5.11}
    r_k^\top p_i = 0, \quad i=0,1,\dots,k-1.
\end{equation}
From Algorithm 5.1, notice that
\begin{equation} \label{5.14e}
    p_i = -r_i + \beta_i p_{i-1} \implies r_i = \beta_i p_{i-1} - p_i.
\end{equation}

Plugging this into $r_k^\top r_i$ yields
\[
    r_k^\top r_i = r_k^\top(\beta_i p_{i-1} - p_i) = \beta_i r_k^\top p_{i-1} - r_k^\top p_i = 0,
\]
as $r_k^\top p_{i-1} = r_k^\top p_i = 0$ by \fullref{5.11}. Note that $r_k^\top r_0 = -r_k^\top p_0 = 0$ by definition of $p_0$. Therefore $r_k^\top r_i = 0$ for $i=0,1,\dots,k-1$.


\end{proof}
\end{problem}

%---------------%
%---Problem 2---%
%---------------%


\begin{problem}%[vskip]
\subsection*{Problem 2}

Prove that the conjugate gradient algorithm, the preliminary version (Algorithm 5.1, page 108 in [NW]), is equivalent to Algorithm 5.2 (CG) (page 112 in [NW]), i.e., that
\[
     \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k},
\]
and
\[
     \beta_{k+1} = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\]



\subsection*{Solution}
\begin{proof}

We wish to show that $\alpha_k$ and $\beta_{k+1}$ from Algorithm 5.1 and Algorithm 5.2 are equivalent. In Algorithm 5.1, we define $\alpha_k$ as
\[
     \alpha_k = -\frac{r_k^\top p_k}{p_k^\top A p_k}.
\]
Notice that we can rewrite the numerator using \fullref{5.14e} as
\[
     r_k^\top p_k = r_k^\top(-r_k + \beta_k p_{k-1}) = -r_k^\top r_k + \beta_k r_k^\top p_{k-1},
\]
and from \fullref{5.11}, we can reduce the expression to
\[
     r_k^\top p_k = -r_k^\top r_k + 0 = -r_k^\top r_k,
\]
for all $i=0,1,\dots,k-1$. Plugging this result into $\alpha_k$ gives
\[
    \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k},
\]
as defined in Algorithm 2. In Algorithm 5.1, we define $\beta_{k+1}$ as
\[
     \beta_{k+1} = \frac{r_{k+1}^\top A p_k}{p_k^\top A p_k}.
\]
Recall that we have
\[
     r_{k+1} = r_k + \alpha_k A p_k \implies \alpha_k Ap_k = r_{k+1} - r_k,
\]
and
\[
     p_k^\top Ap_k = \frac{r^\top_k r_k}{\alpha_k}.
\]
Plugging this into $\beta_{k+1}$ yields
\begin{align*}
    \beta_{k+1} &= \frac{r_{k+1}^\top A p_k}{p_k^\top A p_k}\\
    &= \frac{r_{k+1}^\top \alpha_k A p_k}{r^\top_k r_k}\\
    &= \frac{r_{k+1}^\top \paren{r_{k+1} - r_k}}  {r^\top_k r_k}\\
    &= \frac{r_{k+1}^\top r_{k+1} - r_{k+1}^\top r_k}  {r^\top_k r_k}\\
    &= \frac{r_{k+1}^\top r_{k+1}}  {r^\top_k r_k},   
\end{align*}
where $r_{k+1}^\top r_k = 0$ from question 1. Thus gives $\beta_{k+1}$ from Algorithm 2.



\end{proof}
\end{problem}



%---------------%
%---Problem 3---%
%---------------%


\begin{problem}%[vskip]
\subsection*{Problem 3}

Let $A$ be an $n\times  n$ matrix. A subspace spanned by the columns of an $n \times k$ matrix $B$ is an invariant subspace of $A$ if $A$ maps it into itself, i.e., if $AB \subset \text{span}(B)$. This means that there is a $k \times k$ matrix $C$ such that $AB = BC$. Prove that if a vector $r \in \R^n$ lies in the $k$-dimensional subspace spanned by the columns of $B$, i.e., if $r = By$ for some $y \in \R^k$ ($r$ is a linear combination of columns of $B$ with coefficients $y_1, \dots , y_k$) then the Krylov subspaces generated by $r$ spot expanding at degree $k - 1$, i.e,
\[
     \text{span}\brac{r,Ar,\dots,A^pr} = \text{span}\brac{r,Ar,\dots,A^{k-1}r}, \quad \forall p \geq k.
\]

\subsection*{Solution}
\begin{proof}
    Let $A$ be an $n\times  n$ matrix. A subspace spanned by the columns of an $n \times k$ matrix $B$ is an invariant subspace of $A$ if $A$ maps it into itself, i.e., if $AB \subset \text{span}(B)$. This means that there is a $k \times k$ matrix $C$ such that $AB = BC$. We wish to show that if a vector $r \in \R^n$ lies in the $k$-dimensional subspace spanned by the columns of $B$, i.e., if $r = By$ for some $y \in \R^k$ ($r$ is a linear combination of columns of $B$ with coefficients $y_1, \dots , y_k$) then the Krylov subspaces generated by $r$ spot expanding at degree $k - 1$, i.e,
    \[
        \text{span}\brac{r,Ar,\dots,A^pr} = \text{span}\brac{r,Ar,\dots,A^{k-1}r}, \quad \forall p \geq k.
   \]
    Observe that for $p \leq k$, we have that
    \begin{align*}
        \text{span}\{r,Ar,\dots,A^{p}r\} &= \text{span}(By,ABy,\dots,A^{p}By)\\
        &= \text{span}\{By,BCy,\dots,BC^{p}y\}\\
        &= B\text{span}\{y,Cy,\dots,C^{k-1} y, \dots,C^{p} y\}\\
        &= B\text{span}\{y,Cy,\dots,y,C^{k-1}y\}\\
        &= \text{span}\{By,BCy,\dots,BC^{k-1} y\}\\
        &= \text{span}\{By,ABy,\dots,A^{k-1}By\}\\
        &= \text{span}\{r,Ar,\dots,A^{k-1}r\}\\
    \end{align*} 
    where we use the fact, $A^n B = BC^n$ for $\forall n$ and that a vector $r = By \in \R^k$ lies in the $k$-dimensional subspace spanned by the columns of $B$ so
    \[
     B\text{span}\{y,Cy,\dots,C^{k-1} y, \dots,C^{p} y\} = B\text{span}\{y,Cy,\dots,y,C^{k-1}y\} \quad \forall p \geq k.
    \]
     Therefore,
     \[
        \text{span}\brac{r,Ar,\dots,A^pr} = \text{span}\brac{r,Ar,\dots,A^{k-1}r}, \quad \forall p \geq k.
     \]

    % Clearly 
    % \[
    %     \text{span}\brac{r,Ar,\dots,A^{k-1}r} \subseteq \text{span}\brac{r,Ar,\dots,A^pr}, \quad \forall p \geq k,
    % \]
    % so let's show the converse. Let $x \in \text{span}\brac{r,Ar,\dots,A^pr}$ such that
    % \begin{align*}
    %     x &= \alpha_0 r + \alpha_1 Ar + \cdots + \alpha_p A^p r\\
    %     &=  \alpha_0 By + \alpha_1 ABy + \cdots + \alpha_k A^{k}By + \cdots + \alpha_p A^{p}By, 
    % \end{align*}
    % Now since $AB \subset \text{span}(B)$, we have that $x \in \text{span}(B)$. Since $B$ has rank at most $k$, 






    % Let's prove that the Krylov subspaces generated by the vector $r$ expand at degree $k - 1$, i.e., $\text{span}{r,Ar,\dots,A^pr} = \text{span}{r,Ar,\dots,A^{k-1}r}$ for all $p \geq k$. We'll use the fact that the subspace spanned by the columns of the matrix $B$ is an invariant subspace of $A$.

    % Given that $r = By$ for some $y \in \mathbb{R}^k$, we can express the Krylov subspaces generated by $r$ as follows:
    
    % \begin{align*}
    % \text{span}{r,Ar,\dots,A^pr} &= \text{span}{By, A(By), \dots, A^p(By)} \
    % &= \text{span}{By, ABy, \dots, A^pBy}.
    % \end{align*}
    
    % Now, we need to show that this span is equal to $\text{span}{r, Ar, \dots, A^{k-1}r}$. To do this, let's first write out a few terms in both sets.
    
    % The set ${By, ABy, \dots, A^pBy}$ contains terms like $By, ABy, A^2By, \dots, A^pBy$.
    
    % The set ${r, Ar, \dots, A^{k-1}r}$ contains terms like $r, Ar, A^2r, \dots, A^{k-1}r$.
    
    % We want to show that every term in set 1 can be expressed as a linear combination of terms in set 2 for all $p \geq k$. Since we know that the subspace spanned by the columns of $B$ is an invariant subspace of $A$ (i.e., $AB \subset \text{span}(B)$), we can use this property to express terms in set 1 in terms of set 2.
    
    % Specifically, for any $p \geq k$, we can write:
    
    % \begin{align*}
    % A^pBy &= A^{p-k}A^kBy \quad \text{(splitting into a product of $A^k$ and $A^{p-k}$)} \
    % &= A^{p-k}B(A^kr) \quad \text{(since $r = By$)}.
    % \end{align*}
    
    % Now, notice that $A^{p-k}B(A^kr)$ is in the span of ${r, Ar, \dots, A^{k-1}r}$ because we've expressed it as a product of matrices $A^{p-k}$ and $B$ times a vector $A^kr$ that is in the span of ${r, Ar, \dots, A^{k-1}r}$. Therefore, every term in set 1 can be expressed as a linear combination of terms in set 2 for all $p \geq k$, which means that:
    
    % span
    

\end{proof}
\end{problem}



%---------------%
%---Problem 4---%
%---------------%


\begin{problem}%[vskip]
\subsection*{Problem 4}

Prove Theorem 5.5 From [NW], page 115. Here are the steps that you need to work out.
\begin{enumerate}
    \item [(a)] Construct a polynomial $Q(\lambda)$ of degree $k+1$ with roots $\lambda_n,\lambda_{n-1},\dots,\lambda_{n-k+1}$ and $\frac{1}{2}(\lambda_1 + \lambda_{n-k})$ such that $Q(0) = 1$.
    
    \item [(b)] Argue that $P(\lambda)$ defined as
    \[
         P(\lambda) = \frac{Q(\lambda)-1}{\lambda},
    \]
    is a polynomial, not a rational function, by referring to the theorem about factoring polynomials. Cite that theorem.
    \item[(c)] Use the ansatz
    \[
         \|x_{k+1} - x^*\|_A^2 \leq \min_{P \subset \mathcal{P}_k}\max_{1\leq i \leq n}[1 + \lambda_i P_k(\lambda_i)]^2\|x_0 - x^*\|_A^2.   
    \]
    Argue that 
    \[
        \|x_{k+1} - x^*\|_A^2 \leq \max_{1\leq i \leq n}Q(\lambda_i)^2\|x_0 - x^* \|_A^2.
    \]
    \item[(d)] Show that
    \[
         \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}[Q(\lambda)]^2 \leq \max_{\lambda \in [\lambda_1,\lambda_{n-k}]} \abs{\frac{\lambda - \frac{1}{2}(\lambda_1 + \lambda_{n-k})}{\frac{1}{2}(\lambda_1 + \lambda_{n-k})}}^2.
    \]
    \item[(e)] Find the maximum of the function in the right-hand side of the last equation in the interval $[\lambda_1,\lambda_{n-k}]$.
    \item[(f)] Finish the proof of the theorem.
\end{enumerate}

\subsection*{Solution}
\begin{proof}

We wish to prove that if $A$ has eigenvalues $\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$, then
\[
      \|x_{k+1} - x^* \|_A^2 \leq \paren{\frac{\lambda_{n-k}-\lambda_1}{\lambda_{n-k} + \lambda_1}}^2 \|x_{0} - x^* \|_A^2.
\]
Let's begin by constructing a polynomial $Q(\lambda)$ of the form
\[
      Q(\lambda) = a(\lambda - \lambda_n)(\lambda - \lambda_{n-1})\cdots(\lambda - \lambda_{n-k+1})\paren{\lambda - \frac{\lambda_1 + \lambda_{n-k}}{2}},
\]
then if we want to $Q(\lambda)$ to satisfy $Q(0)=1$ we set
\[
      Q(0) = 1 = a(- \lambda_n)( - \lambda_{n-1})\cdots( - \lambda_{n-k+1})\paren{ - \frac{\lambda_1 + \lambda_{n-k}}{2}}
\]
so
\[
      a = \frac{(-1)^{k+1}}{(\lambda_n)( \lambda_{n-1})\cdots(\lambda_{n-k+1})\paren{\frac{\lambda_1 + \lambda_{n-k}}{2}}}.
\]
Thus
\[
      Q(\lambda) = \frac{(-1)^{k+1}(\lambda - \lambda_n)(\lambda - \lambda_{n-1})\cdots(\lambda - \lambda_{n-k+1})\paren{\lambda - \frac{\lambda_1 + \lambda_{n-k}}{2}}}{(\lambda_n)( \lambda_{n-1})\cdots(\lambda_{n-k+1})\paren{\frac{\lambda_1 + \lambda_{n-k}}{2}}}.
\]
Then if we define
\[
     P(\lambda) = \frac{Q(\lambda) - 1}{\lambda},
\]
which is a rational function by the fundamental theorem of algebra (or the polynomial remainder theorem) as we have written $Q(\lambda)$ completely in terms of its factors, we can also completely factor
\[
      Q(\lambda) - 1 = \lambda P(\lambda),
\]
and since $Q(\lambda) - 1$ is a rational function with a root at $\lambda = 0$, so $R(\lambda)$ must be a rational function of degree $k$ by polynomial division. Now let's assume that
\begin{align} \label{bi boi}
     \|x_{k+1} - x^*\|_A^2 &\leq \min_{R \subset \mathcal{P}_k}\max_{1\leq i \leq n}[1 + \lambda_i R_k(\lambda_i)]^2\|x_0 - x^*\|_A^2 \nonumber \\ 
     &\leq \max_{1\leq i \leq n}[1 + \lambda_i P(\lambda_i)]^2\|x_0 - x^*\|_A^2 \nonumber \\ 
     &= \max_{1 \leq i \leq n}Q(\lambda_i)^2\|x_0 - x^*\|_A^2. 
\end{align}
Now we have that by 
\begin{align*}
     \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}[Q(\lambda)]^2 &= \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\paren{\frac{(-1)^{k+1}(\lambda - \lambda_n)(\lambda - \lambda_{n-1})\cdots(\lambda - \lambda_{n-k+1})\paren{\lambda - \frac{\lambda_1 + \lambda_{n-k}}{2}}}{(\lambda_n)( \lambda_{n-1})\cdots(\lambda_{n-k+1})\paren{\frac{\lambda_1 + \lambda_{n-k}}{2}}}}^2\\
     &\leq\max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{(-1)^{k+1}(\lambda - \lambda_n)(\lambda - \lambda_{n-1})\cdots(\lambda - \lambda_{n-k+1})\paren{\lambda - \frac{\lambda_1 + \lambda_{n-k}}{2}}}{(\lambda_n)( \lambda_{n-1})\cdots(\lambda_{n-k+1})\paren{\frac{\lambda_1 + \lambda_{n-k}}{2}}}}^2\\
     &= \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{(\lambda - \lambda_n)}{\lambda_n}}^2\max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{(\lambda - \lambda_{n-1})}{\lambda_{n-1}}}^2 \cdots \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{\lambda - \frac{1}{2}(\lambda_1 + \lambda_{n-k})}{\frac{1}{2}(\lambda_1 + \lambda_{n-k})}}^2,
\end{align*}
and since by the ordering of $\lambda_i$s, we have that the max of each term is achieved by $\lambda_1$. \footnote{note that $\lambda_{n-k}$ also achieves the max and gives the same results for the rest of this proof.} Furthermore, notice that plugging $\lambda_1$ into all of the terms expect the last term, we will have that each term is less than or equal to one, i.e, 
\[
     \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{(\lambda - \lambda_{n-i})}{\lambda_{n-i}}}^2 = \abs{\frac{(\lambda_1 - \lambda_{n-i})}{\lambda_{n-i}}}^2 \leq 1, \quad \forall 0 \leq i \leq k+1.
\]
Thus we have that
\[
     \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}[Q(\lambda)]^2 \leq \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{\lambda - \frac{1}{2}(\lambda_1 + \lambda_{n-k})}{\frac{1}{2}(\lambda_1 + \lambda_{n-k})}}^2.
\]
Now, as the max of the right-hand term of the above equation is also achieved $\lambda_{1}$, we find that
\[
     \max_{\lambda \in [\lambda_1,\lambda_{n-k}]}\abs{\frac{\lambda - \frac{1}{2}(\lambda_1 + \lambda_{n-k})}{\frac{1}{2}(\lambda_1 + \lambda_{n-k})}}^2 = \abs{\frac{\lambda_{1} - \frac{1}{2}(\lambda_1 + \lambda_{n-k})}{\frac{1}{2}(\lambda_1 + \lambda_{n-k})}}^2 = \abs{\frac{\lambda_1-\lambda_{n-k}}{\lambda_{n-k} + \lambda_1}}^2 = \paren{\frac{\lambda_{n-k}-\lambda_1}{\lambda_{n-k} + \lambda_1}}^2.
\]
Plugging this back into \fullref{bi boi} yields
\[
     \|x_{k+1} - x^* \|_A^2 \leq \paren{\frac{\lambda_{n-k}-\lambda_1}{\lambda_{n-k} + \lambda_1}}^2 \|x_{0} - x^* \|_A^2.
\]



\end{proof}
\end{problem}



%---------------%
%---Problem 5---%
%---------------%


\begin{problem}%[vskip]
\subsection*{Problem 5}

Matlab Problem

\subsection*{Solution}
\begin{proof}

We implemented the conjugate gradient method in MatLab with the script:
\begin{lstlisting}[style=Matlab-editor]
function [res,x] = CG(A,b,tol)
    % input: A,b and tol
    % output: norm of residual at each iteration
    
    %compute x_0
    x = zeros(length(A),1);
    %set up
    r = A*x - b;
    p = -r;
    res = norm(r);
    
    while norm(r) >= tol
        alpha = (r'*r) / (p'*A*p);
        x = x + alpha*p;
        rNew = r + alpha*A*p;
        beta = (rNew'*rNew)/(r'*r);
        p = -rNew + beta*p;
        
        r = rNew;
        res = [res,norm(r)];
    end
end
\end{lstlisting}
and the preconditioned conjugate gradient method as:
\begin{lstlisting}[style=Matlab-editor]
function [res,x] = PCG(A,b,tol)
    % input: A,b and tol
    % output: norm of residual at each iteration
    
    %compute x_0
    x = ones(length(A),1);
    
    %compute M
    ichol_fac = ichol(sparse(A));
    M = ichol_fac*ichol_fac';

    %set up
    r = A*x - b;
    y = M\r;
    p = -y;
    res = norm(r);
    
    while norm(r) >= tol
        alpha = (r'*y) / (p'*A*p);
        x = x + alpha*p;
        rNew = r + alpha*A*p;
        yNew = M\rNew;
        beta = (rNew'*yNew)/(r'*y);
        p = -yNew + beta*p;
        
        y = yNew;
        r = rNew;
        res = [res,norm(r)];
    end
end
\end{lstlisting}
Then using the two methods to solve
\[
      -L_\text{symm}y = -b_\text{symm},
\]
and plotting the norm of the residual at each iteration, see Figure \ref{fig1}, we see that the preconditioned conjugate gradient method converges in significantly fewer iterations than the conjugate gradient. We can also visualize the solutions to see that both methods are able to correctly solve the system and find the exit, see figure \ref{fig2}. Code can be found at \url{https://github.com/MarvynBailly/AMSC660/blob/main/homework7}. 
\begin{figure}[H]
     \centering
     \includegraphics[width=0.75\textwidth,height=\textwidth,keepaspectratio]{normres.png}
     \caption{The 2 norm of the residuals of conjugate gradient method (seen in blue) compared to the preconditioned conjugate gradient method (seen in orange)} at each iteration.
     \label{fig1}
\end{figure}
\begin{figure}[H]
     \begin{subfigure}[b]{0.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{solcg.png}
          \caption{}
          \label{fig2:a}
          \vspace{4ex}
     \end{subfigure}%%
     \begin{subfigure}[b]{0.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{solpcg.png}
          \caption{}
          \label{fig2:b}
          \vspace{4ex}
     \end{subfigure}
     \caption{Visualized solution of the conjugate gradient method (a) and preconditioned conjugate gradient method (b).}
     \label{fig2}
\end{figure}

\end{proof}
\end{problem}












\end{document}